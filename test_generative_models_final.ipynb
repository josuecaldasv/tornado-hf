{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMW2EaAy77mZ",
        "outputId": "36e0aff6-45da-457f-c3f2-40f5b37888fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/phaxssi/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/phaxssi/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import random\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "import tiktoken\n",
        "import os\n",
        "from gpt4all import GPT4All\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l3f0v_DaiOCC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/phaxssi/miniconda3/envs/rag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import functions as fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5CuAUzP8dbA"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/chen700564/RGB/master/data/en.json\"\n",
        "data = fn.process_json(url)\n",
        "data = data[:1]\n",
        "# data = random.sample(data, 1)\n",
        "queries = [item[\"query\"] for item in data]\n",
        "answers = [item[\"answer\"][0] for item in data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyoCdtNBhm6N"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "You are an AI assistant specializing in Question Answering. Your task is to read the provided context carefully and then generate the most accurate and concise answer to the question based on the context.\n",
        "\n",
        "Context: {context_concat}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Es3HdNidom1"
      },
      "outputs": [],
      "source": [
        "device = GPT4All.list_gpus()[0]\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuGUJobRnWJb",
        "outputId": "b353cdb9-97c1-4703-9134-2cee1681e398"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 7.37G/7.37G [12:38<00:00, 9.72MiB/s]\n",
            "Verifying: 100%|██████████| 7.37G/7.37G [00:32<00:00, 228MiB/s]\n",
            "Model downloaded to '/root/.cache/gpt4all/gpt4all-13b-snoozy-q4_0.gguf'\n"
          ]
        }
      ],
      "source": [
        "# gen_model_1 = GPT4All(\"gpt4all-13b-snoozy-q4_0.gguf\", device=\"cuda:Tesla T4\", verbose=True)\n",
        "# gen_model_1.name = \"gpt4all-13b-snoozy-q4_0.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gen_model_1 = GPT4All(\"gpt4all-13b-snoozy-q4_0.gguf\", device = device)\n",
        "gen_model_2 = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\", device = device)\n",
        "gen_model_1.name = \"gpt4all-13b-snoozy-q4-0.gguf\"\n",
        "gen_model_2.name = \"Meta-Llama-3-8B-InstructQ4-0.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uGhwhIDmhm2l",
        "outputId": "7a31bb49-ab1e-421d-fb08-6e2291123487"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]Exception ignored on calling ctypes callback function: <function LLModel._prompt_callback at 0x7e26b246b2e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gpt4all/_pyllmodel.py\", line 601, in _prompt_callback\n",
            "    @staticmethod\n",
            "KeyboardInterrupt: \n",
            "100%|██████████| 5/5 [11:44<00:00, 140.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total execution time: 11.82841622432073 minutes.\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "output_path = '/results/gen'\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Set a seed for reproducibility\n",
        "random.seed(2024)\n",
        "\n",
        "# Define noise thresholds\n",
        "noise_thresholds = {\n",
        "    'Noise_0': 0.0,\n",
        "    'Noise_20': 0.20,\n",
        "    'Noise_40': 0.40,\n",
        "    'Noise_60': 0.60,\n",
        "    'Noise_80': 0.80,\n",
        "    'Noise_100': 1.0\n",
        "}\n",
        "\n",
        "# Models to be tested\n",
        "models = [gen_model_1, gen_model_2]\n",
        "separator = \" <|> \"\n",
        "max_total_tokens = 1600\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "# Number of experiments\n",
        "num_experiments = 5\n",
        "\n",
        "for model in models:\n",
        "    for exp_num in range(1, num_experiments + 1):\n",
        "        random.seed(2024 + exp_num)\n",
        "\n",
        "        results = []\n",
        "        for query, positive_context, negative_context, answer in tqdm(zip(queries, [item[\"positive\"] for item in data], [item[\"negative\"] for item in data], answers), total=len(queries)):\n",
        "            result = {\n",
        "                'Query': query,\n",
        "                'Correct Answer': answer,\n",
        "            }\n",
        "\n",
        "            with model.chat_session():\n",
        "                for noise_level, label in noise_thresholds.items():\n",
        "                    mixed_context = fn.create_mixed_context(positive_context, negative_context, label, max_total_tokens, separator)\n",
        "                    context_concat = separator.join(mixed_context)\n",
        "                    prompt = prompt_template.format(context_concat=context_concat, query=query)\n",
        "                    generated_answer = model.generate(prompt)\n",
        "                    result.update({f'{noise_level} Predicted Answer': generated_answer})\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "        results_df = pd.DataFrame(results)\n",
        "\n",
        "        for label in noise_thresholds.keys():\n",
        "            results_df[f'EM {label}'] = results_df.apply(lambda row: fn.apply_exact_match(row, f'{label} Predicted Answer', 'Correct Answer'), axis=1)\n",
        "            results_df[f'Jaccard {label}'] = results_df.apply(lambda row: fn.apply_jaccard(row, f'{label} Predicted Answer', 'Correct Answer'), axis=1)\n",
        "            results_df[f'Cosine {label}'] = results_df.apply(lambda row: fn.apply_cosine(row, f'{label} Predicted Answer', 'Correct Answer'), axis=1)\n",
        "            results_df[f'EM - 2V {label}'] = results_df.apply(lambda row: fn.apply_exact_match_2v(row, f'{label} Predicted Answer', 'Correct Answer'), axis=1)\n",
        "\n",
        "        # Save results to a file with the experiment number and model name\n",
        "        filename = os.join(output_path, f\"exp_{exp_num}_{model.name}.json\")\n",
        "        results_df.to_json(filename, orient='records', lines=True)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Total execution time: {(end_time - start_time) / 60} minutes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VmZBYMmKiPVZ"
      },
      "outputs": [],
      "source": [
        "# input_paths = '/content/drive/My Drive/tornado-tasks/results/gen/'\n",
        "# files = os.listdir(input_paths)\n",
        "\n",
        "# for file in files:\n",
        "#     if file.endswith('.json'):\n",
        "#         print(file)\n",
        "#         model_name = file.split('.')[0]\n",
        "#         full_path = os.path.join(input_paths, file)\n",
        "#         for data_chunk in fn.read_json_in_chunks(full_path, fn.cols_to_use, chunk_size=1000):\n",
        "#             with open(f'/content/drive/My Drive/tornado-tasks/text/gen/{model_name}.txt', 'a') as output_file:\n",
        "#                 print(model_name)\n",
        "#                 for part in fn.format_results(data_chunk):\n",
        "#                     output_file.write(part + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8_JZXmNtCptZ",
        "outputId": "9316c974-e3da-4c83-c570-5e5c6af2ba16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing files:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpt4all-13b-snoozy-q4_0.gguf.json\n",
            "/content/drive/My Drive/tornado-tasks/results/gen/gpt4all-13b-snoozy-q4_0.gguf.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files: 100%|██████████| 1/1 [00:05<00:00,  5.60s/it]\n"
          ]
        }
      ],
      "source": [
        "threshold = 0.8\n",
        "input_paths = '/results/gen'\n",
        "output_path = '/metrics/gen'\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "output_file = os.join(output_path, 'all_metrics.xlsx')\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "files = os.listdir(input_paths)\n",
        "\n",
        "for i, file in enumerate(tqdm(files, desc=\"Processing files\"), start=1):\n",
        "    if file.endswith('.json'):\n",
        "        experiment_num = int(file.split('_')[1])\n",
        "        print(f'Experiment: {experiment_num}')\n",
        "        model_name = file.split('_')[2].replace('.json', '')\n",
        "        sheet_name = model_name\n",
        "        print(f\"Processing file {i}/{len(files)}: {file}\")\n",
        "        input_path = os.path.join(input_paths, file)\n",
        "        result_df = fn.compute_metrics(input_path, threshold)\n",
        "        result_df.insert(0, 'Experiment Number', experiment_num)\n",
        "\n",
        "        if sheet_name not in all_results:\n",
        "            all_results[sheet_name] = result_df\n",
        "        else:\n",
        "            all_results[sheet_name] = pd.concat([all_results[sheet_name], result_df], ignore_index=True)\n",
        "\n",
        "with pd.ExcelWriter(output_file) as writer:\n",
        "    for sheet_name, result_df in all_results.items():\n",
        "        result_df.to_excel(writer, sheet_name=sheet_name, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_file = '/metrics/gen/all_metrics.xlsx'\n",
        "output_file = '/metrics/gen/final_metrics.xlsx'\n",
        "\n",
        "# Leer el archivo Excel original\n",
        "excel_data = pd.read_excel(input_file, sheet_name=None)\n",
        "\n",
        "final_results = {}\n",
        "\n",
        "# Procesar cada hoja del archivo Excel\n",
        "for sheet_name, df in excel_data.items():\n",
        "    # Asegurarse de que la columna 'Experiment Number' esté presente\n",
        "    if 'Experiment Number' not in df.columns:\n",
        "        raise ValueError(f\"'Experiment Number' column not found in sheet {sheet_name}\")\n",
        "\n",
        "    # Identificar las métricas y niveles de ruido\n",
        "    metrics = df['Metric'].unique()\n",
        "    noise_levels = ['Noise_0', 'Noise_20', 'Noise_40', 'Noise_60', 'Noise_80', 'Noise_100']\n",
        "\n",
        "    # Crear DataFrame para resultados finales\n",
        "    result_data = {\n",
        "        'Metric': metrics,\n",
        "    }\n",
        "\n",
        "    # Calcular la media y desviación estándar para cada métrica y nivel de ruido\n",
        "    for noise_level in noise_levels:\n",
        "        result_data[f'{noise_level}_Mean'] = []\n",
        "        result_data[f'{noise_level}_Std'] = []\n",
        "        for metric in metrics:\n",
        "            metric_df = df[df['Metric'] == metric]\n",
        "            result_data[f'{noise_level}_Mean'].append(metric_df[noise_level].mean())\n",
        "            result_data[f'{noise_level}_Std'].append(metric_df[noise_level].std())\n",
        "\n",
        "    result_df = pd.DataFrame(result_data)\n",
        "    final_results[sheet_name] = result_df\n",
        "\n",
        "# Guardar los resultados en un nuevo archivo Excel\n",
        "with pd.ExcelWriter(output_file) as writer:\n",
        "    for sheet_name, result_df in final_results.items():"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
